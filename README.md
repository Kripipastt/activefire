# Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study


## Authors

[Gabriel Henrique de Almeida Pereira](https://github.com/pereira-gha)

[Andre Minoro Fusioka](https://github.com/Minoro)

[Bogdan Tomoyuki Nassu](https://github.com/btnassu)

[Rodrigo Minetto](https://github.com/rminetto)


# Dataset and Weights

You will find all the dataset and trained weights inside a directory on [Google Drive](https://drive.google.com/drive/folders/1GIcAev09Ye4hXsSu0Cjo5a6BfL3DpSBm). Inside the [weights](https://drive.google.com/drive/folders/1btWS6o-ZbwYqnnA2p31DIMV_lJ4qHYH2) folder you will find all the trained weights, while inside the [dataset](https://drive.google.com/drive/folders/1FtW6TTl2BrNbYwhGUrSLZtrcv4zOX9tT) folder you will find all images and masks patches. The dataset folder include a [manually annotated dataset](https://drive.google.com/drive/folders/1M4GDDQuHYbbyjdIk9bqOxpUXHz-2nXtc) and a folder named [continents](https://drive.google.com/drive/folders/1IZ-Qebb-df2DFxfuSc2fXMRgd7493PDi) that holdes the images patches and masks of active fire generated by different algorithm. 

We also provide a small subset of our dataset, randomly selected, with some patches and masks for thoso who want to try the code, but can't download the full dataset. This samples are also hosted on [Google Drive](https://drive.google.com/drive/folders/1HD34RJMlRN-XZgNYTu3PBOJ2FYyQoOe4), but in a different shared folder.

**Atention:** the folder named "dataset" in this repository does not contain the images used in the work. The scripts in this repository expect the images, once downloaded, to be placed in specific directories within that folder.

The following sections describe how you can use the files hosted on [Google Drive](https://drive.google.com/drive/folders/1GIcAev09Ye4hXsSu0Cjo5a6BfL3DpSBm) with this repository.

## Downloading the Full Dataset

Our dataset are divided in 11 regions, where each region has a zip file that contains the processed patches and the masks generated by [Kumar and Roy (2018)](https://doi.org/10.1080/17538947.2017.1391341), [Murphy et al. (2016)](https://doi.org/10.1016/j.rse.2016.02.027), [Schroeder  et  al.  (2016)](https://doi.org/10.1016/j.rse.2015.08.032) conditions. We also include masks generated by the combinations of this methods, using the intersection of them and the voting (at least two methods must agree where fires occurs).

You can download it right through [Google Drive](https://drive.google.com/drive/folders/1FtW6TTl2BrNbYwhGUrSLZtrcv4zOX9tT), or if you want, you can use the `src/utils/download_dataset.py` script. If you want only a specific region (or only some regions) you can change the `REGIONS` constant. This script will download the ziped files to `<your-local-repository>/dataset/compressed/`, if you want to use another direcotry, change the `OUTPUT_DIR` constant to point to the directory where the files should be saved. Before running the script you will need the [gdown](https://github.com/wkentaro/gdown) library, you can follow the install instructions on the library page. After you set the constants to the desired values and installed the `gdown` library you can run the script using: 

```
python download_dataset.py
```
**Atention:** Due to the dataset size you may face a error message with the text: "Too many users have viewed or downloaded this file recently", unfortunatly it's seems to be an [error without any fix](https://github.com/wkentaro/gdown/issues/26), you can try to wait or to download the files manually.

After you downloaded the desired regions you can uncompress it running:

```
python unzip_patches.py
```

This script will unzip the downloaded files and separete the masks from the images patches. The images patches will be unziped to `<your-local-repository>/dataset/images/patches` folder, while the masks will be putted in different folders inside `<your-local-repository>/dataset/images/masks`. The masks generated by the mentioned algorithms will be unziped to `<your-local-repository>/dataset/images/masks/patches`, while the masks produced by the intersection of them will be unziped to `<your-local-repository>/dataset/images/masks/intersection` and finaly the masks generated by the voting approach will be puted in `<your-local-repository>/dataset/images/masks/voting/`. This structure are used to train and evaluate the CNN models, if you change this structure be sure of your training and evaluation scripts are pointing to the right directories.

We provide only the processed patches, with their respective masks, but if you want to download the original images, used to generate the patches, and process by yourself, the code inside the folder `src/landsat` gives you an easy way to get data from Landsat-8 satellite available on AWS.

### Downloading the small subset of our dataset

If you can't download the full dataset (or don't want to), you can download a small subset of our dataset to try this  this repository's code. Keep in mind that those samples represents only a small part of our dataset and were randomly selected, because of that you may find differents results from those described in our work.

You can download these samples directly from [Google Drive](https://drive.google.com/drive/folders/1HD34RJMlRN-XZgNYTu3PBOJ2FYyQoOe4) or use the `src/utils/download_dataset.py` script, before running the script you need to set the `DOWNLOAD_FULL_DATASET` to `False`, and then run:
```
python download_dataset.py
```

This will download the compressed samples to `<your-local-repository>/dataset` folder. Notice that the output folder is different from the one used to download the full dataset, this is done to avoid any errors when decompressing the samples if you also downloaded the full dataset. If you want to download this samples to another folder you can set the `OUTPUT_SAMPLES` constant before running the script.

Once you have downloaded the samples, you can uncompress them using the `src/utils/unzip_patches` script. You just need to set the constant `FULL_DATASET` to `False` and run it with: 

```
python unzip_patches.py
```

# Downloading the Manually Annotated dataset

We also provided a manually annotated dataset, croped in patches (`manual_annotations_patches.zip`), the masks generated by the literature algorithms and the combination of them (`masks_patches.zip`), and also the patches from the original Landsat-8 image (`landsat_patches.zip`). You can access the files on [Google Drive](https://drive.google.com/drive/folders/1Gv96zhQ0HwIyquDL8vroarHoz_SJXtv5). You can use the `src/utils/download_manual_annotations.py` script to download these files, this script uses [gdown](https://github.com/wkentaro/gdown), you need install it before using this script. The files will be downloaded to `<your-local-repository>/dataset/manual_annotations/compressed` folder, if you want to save the files in a different directory you can change the `OUTPUT_DIR` constant. To use the download script navigate to `src/utils` folder and run:

```
python download_manual_annotations.py
```

After the download of the zip files with the patches, you can unzip them with the script `src/utils/unzip_manual_annotations.py.py`. If you change the download directory make sure you set the `INPUT_DIR` constant to the same folder. You can also define where the files will be unziped in the constant `OUTPUT_DIR`, the default value is the folder `dataset/manual_annotatios`. This script will create a folder named `patches` that will contains another three folder, one for the images (named `images`), one for the manuall annotations patches (named `annotations`) and another one for the masks generated by the algorithms (named `masks`). To unzip the files, go inside the `src/utils/` directory you can run:

```
python unzip_manual_annotations.py.py
```
<!---
If you prefer, you can download the original scene captured by the Landsat-8 satellited used to do the manual annotations, this images are also available on [Google Drive](https://drive.google.com/drive/folders/1Pmf2mXLhN65_z6YPOi16GEvMGnNTsghD), as well as the masks and manual annotations to the entire scene. You can also use the script `src/utils/download_manual_annotations.py` to download this scenes, you just need to set the constant `DOWNLOAD_SCENES` to `True` before running it. The script `src/utils/unzip_manual_annotations.py.py` can also be used to uncompress the downloaded files, you just need to set constant `UNZIP_SCENES` to `True`, and the files will be unziped to `dataset/manual_annotatios/scenes` directory. You can crop the original scenes in patches with the script `src/utils/crop_manual_annotations.py`, you just need to set the constants `INPUT_ANNOTATION_SCENE_DIR` to the directory where the unziped scenes are and the `OUTPUT_ANNOTATION_PATCHES` to the directory where you want to store the patches.
-->

## Models

Each folder inside the `src/train` directory contains the CNNs approachs trained with different active fire detection algorithms. The code inside these folders are almost the same, the only existing changes are in the constants that configure the CNN (number of filters, number of channes, masks used, etc).

The masks used to train the model are obtained through some well-known active fire detection algorithms and the combination of them. The models inside `kumar-roy` folder were trained with the masks generated by [Kumar and Roy (2018)](https://doi.org/10.1080/17538947.2017.1391341) conditions. The `murphy` folder use the [Murphy et al. (2016)](https://doi.org/10.1016/j.rse.2016.02.027) conditions. And the `schroeder` folder use the [Schroeder  et  al.  (2016)](https://doi.org/10.1016/j.rse.2015.08.032) conditions. The `intersection` and `voting`folder use the masks generated by the combination of the three previous masks, while the first use the masks generated by the intersection of them (all masks must agree where fire occurs), the second use the masks generated by the agreement of at least two of them.

The folder named `unet_16f_2conv_762` (aka. U-Net-Light (3c)) means that the model used is a `U-net` starting with 16 convolutional filters, with 2 convolutions per block, using the channels 7, 6 and 2 from the source image. The folder named `unet_64f_2conv_762` (aka. U-Net (3c)) starts with 64 convolutional filters and use the channels 7, 6 and 2 from the source image. And the folder named `unet_64f_2conv_10c` (aka. U-Net (10c) ) is pretty much the same, but starting with 64 convolutional filters in the first convolutional block and using the 10 channels.


## Downloading the pre-trained weights

Besides the full dataset, you can download the weights produced by our experiments. The weights are avilable on [Google Drive](https://drive.google.com/drive/folders/1btWS6o-ZbwYqnnA2p31DIMV_lJ4qHYH2). The weights are grouped by the masks used, inside each zip you will find the weights produced by each model.

This repository include the `src/utils/download_weights.py` script that will download the weights to `<your-local-repository>/weights` folder. If you want to change the directory where the weights are downloaded, change the `OUTPUT_DIR` constant. Besides, if you don't want to download all weights you can remove the undesired ones from `WEIGHT_FILES` constant.

This script needs the [gdown](https://github.com/wkentaro/gdown) library, if you don't have it yet, follow the library's [install instructions](https://github.com/wkentaro/gdown#installation). After you install it, you can navigate to `src/utils/` and run:

```
python download_weights.py
```

After the download of the weights, you need to move it to the right directories, for this purpouse you can use the `src/utils/unzip_download_weights.py` script. If you change the download directory, you must set the `ZIPED_WEIGHTS_DIR` constant to the right directory. To copy the weights to the trains directories you can set the `UNZIP_TO_TRAIN` constant to `True`, and to copy the weights to the manual annotations evaluation directories you can set `UNZIP_TO_MANUAL_ANNOTATIONS_CNN` to `True`. To run the script navigate to `scr/utils` folder and run:

```
python unzip_download_weights.py
```

This script will put the samples in the expected directories to run the code of this repository, but you can change the output directories changing the values of the constants `IMAGES_PATH`, `MASKS_PATH` and `MANUAL_ANNOTATIONS_PATH`.

## Sampling the data

If you decide to train the networks from scratch you will need to separete the samples in three sets. This repository include the CSV files used in our experiments, this means that not all images and masks may be found in the [subset of samples](https://drive.google.com/drive/folders/1HD34RJMlRN-XZgNYTu3PBOJ2FYyQoOe4) and if you don't download all the dataset you will need to generate a new CSV files.

The samples used for training, test and validation are defined by CSV files inside the model folder named `dataset` (e.g `src/train/kumar-roy/unet_16f_2conv_762/dataset`). The file `images_masks.csv` list all images and the corresponding mask for the approach. The `images_train.csv` and `masks_train.csv` list the files used to train the model, the `*_val.csv` hold the files for the validation and the `*_test.csv` has the files used in the test phase.

You can create new CSV files to use, for this purpose you will find the script in `src/utils/split_dataset.py` that will split your data in three different set.

You may change the constans `IMAGES_PATH` and `MASKS_PATH` with the folder that hold your images and masks. You also need to change the `MASK_ALGORITHM` with the name of the algorithm that created the masks (Schroeder, Murphy, Kumar-Roy, Intersection or Voting), this constant will define with image and mask will be included in the CSV files. If you are tranining a `src/train/kumar-roy/unet_*` model set it to `Kumar-Roy`, if you are traning a `src/train/murphy/unet_*` set it to `Murphy`, etc.

Inside the `src/utils` directory run:
```
python split_dataset.py
```

This will create the `images_masks.csv`, the`images_*.csv` and the `masks_*.csv`. For consistent experiments you need to copy this files inside all models of an approach. So, if you set the `MASK_ALGORITHM` with `Kumar-Roy`, for example, all `src/train/kumar-roy/unet_*/dataset` must have the same files.

By default the data will be divided in a proportion of 40% for training, 50% for testing and 10% for validation. If you want to change this proportion you need to change the `TRAIN_RATIO`, `TEST_RATIO` and `VALIDATION_RATIO`.

## Training

If you want to train the model from scratch you need to navigate to the desired folder (e.g `src/train/kumar-roy/unet_16f_2conv_762`) and simply run:

```
python train.py
```

This will execute all steps needed to train a new model. You may change the constant `CUDA_DEVICE` to the number of the GPU you want to use. This code expectes that the samples are in a sibling folder of `src` named `dataset`, the images must be in the `dataset/images/patches` and the masks in `dataset/masks/patches`. The artifical masks use a different folder: `dataset/masks/intersection` for intersection masks and `dataset/masks/voting` for voting masks. If you are using other directory to hold your samples you may change the `IMAGES_PATH` and `MASKS_PATH` constants.

The output produced by the training script will be placed at `train_output` folder inside the model folder. This repository already include trained weights inside this folder for the U-Net-Light (3c) models, so if you retrain the model **this weights will be overwrited**.

Besides the final weights, this script will save checkpoints every 5 epochs, if you need to resume from a checkpoint you just need to set the constant `INITIAL_EPOCH` with the epoch corresponding to the checkpoint.

## Testing the trained models

**Attention:** this process will fit all data in your RAM, so this can freeze your machine. If you are running this code in a low memory environment you can reduce the rows in the `images_test.csv` and `masks_test.csv` before these steps.

The testing phase are divided in two main steps. The first one is to apply the trained model over the `images_test.csv` images and save the output as a txt file, where 0 represents background and 1 represents fire. The masks in the `masks_test.csv` will also be converted in a txt file. These files will be written in the `log` folder inside the model folder. The output prediction produced by the CNN will be saved with the name `det_<image-name>.txt` while the corresponding mask will be saved with the name `grd_<mask-name>.txt`. To execute this process run:

```
python inference.py
```

You may change the constant `CUDA_DEVICE` to the number of the GPU you want to use. If your samples are placed in other diretory than the default you need to change the constant `IMAGES_PATH` and `MASKS_PATH`. The output produced by the CNN are converted to interger through a thresholding process, the default threshold are `0.25` you can change this value in the `TH_FIRE` constant.

After this processes you can start the second step to evaluate your trained model. You can simply run:

```
python evaluate_v1.py
```

This will show the results to your model.

If you face the message `[ERROR] Dont match X - Y` this means that a mask or a prediction is missing. Make sure all predictions produced by your model have a corresponding mask.


## Compare against the Manual Annotations

The code inside the folder `src/manual_annotations` gives you a way to compare the masks of the algorithms and the output produced by CNN against the manual annotations. To compare the algorithms (Schroeder, Murphy, Kumar-Roy, Intersection and Voting) against the manual annotations run the following two commands:

```
python inference.py
python evaluate_v1.py
```

The first script will convert the masks and the manual annotations to txt files, inside the `src/manual_annotations/log` folder. The script name `inference.py` was kept for consistency, but no CNN is used in the process. The second script will apply the metrics over the txt files, this will fit all files in your machine's memory. After it ends, you will see the results on the screen.

In the same way, you may test the output produced by the CNN against the groundtruth. The code inside `src/manual_annotatios/cnn_compare` gives you a way to compare each model. It is important to copy the trained weights inside a folder named `weights` inside each model folder. Alternatively you can change the constant `WEIGHTS_FILE` with the desired weights. If you downloaded the pre-trained weights and unziped them as described in the previous sections the weights are already in the correct directories.

You can compare each model individually, for this access the desired model folder and run:

```
python inference.py
python evaluate_v1.py
```

Or you can run all tests running the `src/groundtruth/cnn_compare/run_all.sh` script. This will create a `txt` file inside each model folder with the results.


## Show the CNN output

You can also generate images from trained models. For this purpose you can access `src/utils/cnn` folder and you will find the `generate_inference.py` script. This script will apply the selected model over an image patch and generate a PNG image with the prediction. You need to define the constant `IMAGE_NAME` with the desired image, and if set the constant `IMAGE_PATH` with the path where this image can be found. It is important to define the constant `MASK_ALGORITHM` with the approach you want to use, the `N_CHANNELS` and `N_FILTERS` with the number of channels used and number of filters of the model. Make sure that the trained weights defined in `WEIGHTS_FILE` is consistent with the parameters defined. After that you can run:

```
python generate_inference.py
```

You may want to compare the output produced by the different architectures and also the masks. The script `generate_figures.py` applies the different networks trained on all patches from the image defined in the constant `IMAGE_NAME`. The output folder will have the output produced from the CNNs, the masks available and an image path with the combination of channels 7, 6 and 2 in a PNG format. This script can be run using:
```
python generate_figures.py
```

## Useful stuff

In the `src/utils` you will find some scripts that may help you understant the images and masks you are working with. For example, you may want to know how many fire pixels are in a mask, for this you can use the `count_fire_pixels.py` script. You just define the `IMAGE_NAME` with the desired mask name and run:

```
python count_fire_pixels.py
```

Alternatively you can define a partial name in the constant `IMAGE_NAME` and the `PATCHES_PATTERN` with a pattern to be found and count fire pixels from many patches.

You may need to find an image with at least some minimal amount of fire pixels, this repository has the script `masks_with_at_least_n_fire_pixels.py` that will print on the screen the image path of the masks in the `MASK_PATH` directory that has more fire pixels than `NUM_PIXELS` defined in the script. After defining the path and the amount of pixels you are intered you can run:
```
python masks_with_at_least_n_fire_pixels.py
```

The masks has the value 1 where fire occurs and 0 otherwise, because of that the masks will not display "white" and "black" if oppened. To help you see the mask you can use the script `transform_mask.py`, this script will convert the image to a PNG with white where fire occurs with a black background. You just need to define the mask you want to convert in the constant `MASK_PATH` and run:

```
python transform_mask.py
```

The images available in the dataset are also difficult to view, as they are images with 10 channels. You can convert them into a visible format, with the combination of bands 7, 6 and 2, using the `convert_patch_to_3channels_image.py` script. You just need to set the path to the desired image in the constant `IMAGE` and run:

```
python convert_patch_to_3channels_image.py
```

If you trained the models from scratch and want to use those weights to compare against the manual annotations you cam use the script `copy_trained_weights_to_manual_annotations_evaluation.py` to copy the trained weights to the directories used to evaluate the models against the manual annotations:

```
python copy_trained_weights_to_manual_annotations_evaluation.py
```

The evaluation processes will generate many txt files, you can use the `purge_logs.py` script to erase them:

```
python purge_logs.py
```

## Citation

If you find our work useful for your research, please [cite our paper](https://arxiv.org/abs/2101.03409):

```
@misc{pereira2021active,
      title={Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study}, 
      author={Gabriel Henrique de Almeida Pereira and André Minoro Fusioka and Bogdan Tomoyuki Nassu and Rodrigo Minetto},
      year={2021},
      eprint={2101.03409},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
